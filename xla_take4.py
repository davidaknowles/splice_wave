import torch_xla
from torch_xla import runtime as xr
import torch_xla.core.xla_model as xm
import torch_xla.distributed.parallel_loader as pl
import torch_xla.distributed.xla_multiprocessing as xmp

import time
import itertools

from pathlib import Path

import torch
import torchvision
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F

import transcript_data
import spliceAI
import numpy as np
import os
import tcn

import time

class CPURateTracker:
    def __init__(self):
        self.reset()

    def reset(self):
        self.start_time = time.time()
        self.count = 0

    def add(self, n=1):
        self.count += n

    def rate(self):
        elapsed_time = time.time() - self.start_time
        return self.count / elapsed_time if elapsed_time > 0 else 0.0

class TrainXLADDP():
    
    def __init__(
        self, 
        use_xla = False, 
        batch_size = 20, 
        data_parallel = False, 
        num_workers = 0,
        sequence_len = 10000
    ):
    
        self.device = xm.xla_device() if use_xla else "cpu" 
        self.batch_size = batch_size
        self.sequence_len = sequence_len
        self.data_parallel = data_parallel
        
        self.model = spliceAI.SpliceAI_10k(
            in_channels = 5, 
            out_channels = 4, 
            n_embed = 64
        ).to(self.device)
        
        train_dataset = torch.utils.data.TensorDataset( *torch.load("data_cache.pt") )
        train_dataloader = torch.utils.data.DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True, 
            num_workers = num_workers,
            drop_last = True
        )
        
        self.train_device_loader = pl.MpDeviceLoader(train_dataloader, self.device) if self.xla else train_dataloader
        
        # could use bigger batch here but want to be consistent with mamba
        #test_dataloader = transcript_data.get_dataloader(get_gene, test_chroms, receptive_field = 5000, batch_size = 1, device = "cpu", max_len = 30000 )
        #self.test_device_loader = pl.MpDeviceLoader(test_dataloader, self.device)
        
        self.optimizer = torch.optim.Adam(self.model.parameters())

    @property
    def xla(self): 
        return not str(self.device) == "cpu" # TODO: handle CUDA
    
    def _train_update(self, step, loss, tracker, epoch, onehot):
        print(f'epoch: {epoch}, step: {step}, loss: {loss}, rate: {tracker.rate()} shape: {onehot.shape}')
    
    def train_loop_fn(self, epoch):

        rf = self.model.receptive_field
        
        tracker = xm.RateTracker() if self.xla else CPURateTracker()
        self.model.train()

        from pathlib import Path
        cache_dir = Path("data_cache")
        
        for step_i, (is_exon, one_hot) in enumerate(self.train_device_loader): 

            self.optimizer.zero_grad()

            # maybe not needed now? 
            is_exon = is_exon.nan_to_num() # length 10000
            one_hot = one_hot.nan_to_num() # length 20000
            
            # convert to B x C x T (CNN) from B x T x C (RNN/transformer)
            # these are generated by rnn.pad_sequence internally
            one_hot = one_hot.permute(0, 2, 1) 
            is_exon = is_exon.permute(0, 2, 1)

            one_hot_masked = one_hot.clone()
            #seq_mask = transcript_data.get_mask(self.batch_size, self.sequence_len + rf*2, one_hot_masked, min_span = 1, max_span = 10, mask_same = False)
            # seq_mask will be bool and B x T. 
            #seq_mask_ = seq_mask[:, None, rf:-rf].float().expand(-1, 4, -1)
            
            #seq_mask = torch.rand(self.batch_size, self.sequence_len + rf*2) < 0.15
            #one_hot_masked = one_hot_masked * ~seq_mask[:,None,:] # need to con
            
            seq_mask_ = seq_mask[:, None, rf:-rf].float().expand(-1, 4, -1)
                        
            meta = F.pad(is_exon, (rf,rf)) 
            
            x = torch.concat( (meta, one_hot_masked), 1)
            
            output = self.model(x.nan_to_num()) # spliceAI uses conv which want B x C x T

            one_hot_sub = one_hot[:, :, rf:-rf]
            
            seq_out_norm = output - output.logsumexp(1, keepdims = True)
            loss = - (seq_mask_ * one_hot_sub * seq_out_norm).sum() / ((seq_mask_ * one_hot_sub).sum() + 1e-8)
                        
            loss.backward()
            xm.optimizer_step(self.optimizer) if self.data_parallel else self.optimizer.step()


            tracker.add(self.batch_size)

            if step_i % 10 == 0:
                if self.xla:     
                    xm.add_step_closure(self._train_update, args=(step_i, loss, tracker, epoch, one_hot))
                else: 
                    self._train_update(step_i, loss, tracker, epoch, one_hot)

    def print(self, *args, **kwargs): 
        xm.master_print(*args, **kwargs) if self.xla else print(*args, **kwargs)
    
    def train(self):
        
        for epoch in range(1):
            self.print('Epoch {} train begin {}'.format(epoch, time.strftime('%l:%M%p %Z on %b %d, %Y')))
            self.train_loop_fn(epoch)
            self.print('Epoch {} train end {}'.format(epoch, time.strftime('%l:%M%p %Z on %b %d, %Y')))
        
        if self.xla: 
            xm.wait_device_ops()
        
def _mp_fn(index, flags):
    # "cpu" # xm.xla_device() # == torch_xla.device()
    xla_ddp = TrainXLADDP(**flags)
    xla_ddp.train()

if __name__ == '__main__':

    if False: 
        flags = { 
            "use_xla" : False,
            "batch_size" : 2, 
            "data_parallel" : False,
            "num_workers" : 0
        } 
    else:     
        flags = { 
            "use_xla" : True,
            "batch_size" : 100, 
            "data_parallel" : True,
            "num_workers" : 0
        } 

    print(flags)
    
    if flags["data_parallel"]: 
        xmp.spawn(_mp_fn, args=(flags,))
    else: 
        _mp_fn(0, flags)

