# -*- coding: utf-8 -*-
"""Test cadaceus

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-QRCoEiCmd359JAiqfK4_DNMUxWjXJgT
"""

from transformers import AutoModelForMaskedLM, AutoTokenizer
import torch
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device available:", device)

model_name = "kuleshov-group/caduceus-ps_seqlen-131k_d_model-256_n_layer-16"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True).to(device)

def mutate_sequence(seq, pos, alt):
    alt_seq = list(seq)
    alt_seq[pos] = alt
    return ''.join(alt_seq)

# approach one: compute something like likelihood on both sequences
ref_seq = "ATGCTGGTCC"
snp_pos = 5
alt_allele = "T"

alt_seq = mutate_sequence(ref_seq, snp_pos, alt_allele)
alt_seq

input = torch.tensor( tokenizer([ref_seq, alt_seq])["input_ids"], device = device)

output = model(input)

logits = output["logits"]

loss = F.cross_entropy(logits.permute(0,2,1), input, reduction = "none")

loss_sum = loss.sum(1)
(loss_sum[1] - loss_sum[0]).item()

# approach two: compute something like likelihood on both sequences
masked_seq = mutate_sequence(ref_seq, snp_pos, "N")

masked_input = torch.tensor( tokenizer([masked_seq])["input_ids"], device = device)

output = model(masked_input)

logits = output["logits"].permute(0,2,1)

loss = F.cross_entropy(logits.repeat(2,1,1), input, reduction = "none")
loss # all the same apart from at SNP

loss_sum = loss.sum(1)
(loss_sum[1] - loss_sum[0]).item()

ref_id = tokenizer(ref_seq[snp_pos])["input_ids"][0]
alt_id = tokenizer(alt_allele)["input_ids"][0]

# think the difference between this and the above is just numerical noise?
(logits[0, alt_id, snp_pos] - logits[0, ref_id, snp_pos]).item()

