import torch_xla
import torch_xla.utils.utils as xu
from torch_xla import runtime as xr
import torch_xla.core.xla_model as xm
import torch_xla.distributed.parallel_loader as pl
import torch_xla.distributed.xla_multiprocessing as xmp

import time
import itertools

import torch
import torchvision
import torch.optim as optim
import torch.nn as nn

import transcript_data
import spliceAI
import numpy as np
import os
import tcn

class TrainXLADDP():
    
    def __init__(self, device = "cpu", batch_size = 20, data_parallel = False):
    
        self.device = device 
        self.batch_size = batch_size
        self.data_parallel = data_parallel
        
        train_dataloader = xu.SampleGenerator(
            data=(
                torch.rand(self.batch_size, 10000, 1), # meta == is_exon
                torch.rand(self.batch_size, 20000, 4)), # one_hot
            sample_count = 1000)
        
        self.model = spliceAI.SpliceAI_10k(
            in_channels = 6, 
            out_channels = 4, 
            n_embed = 64
        ).to(self.device)
        
        train_chroms = ["chr%i" % i for i in range(2,23)] + ["chrX"]
        test_chroms = ["chr1"]
        
        
        self.train_device_loader = pl.MpDeviceLoader(train_dataloader, self.device) if self.xla else train_dataloader
        
        self.optimizer = torch.optim.Adam(self.model.parameters())

    @property
    def xla(self): 
        return not str(self.device) == "cpu" # TODO: handle CUDA
    
    def _train_update(self, step, loss, tracker, epoch, shape):
        print(f'epoch: {epoch}, step: {step}, loss: {loss}, rate: {tracker.rate()} shape: {shape}')
    
    def train_loop_fn(self, epoch):

        rf = self.model.receptive_field
        
        if self.xla: 
            tracker = xm.RateTracker()
        self.model.train()
        
        for step_i, (is_exon, one_hot) in enumerate(self.train_device_loader): 
            self.optimizer.zero_grad()
            
            # convert to B x C x T (CNN) from B x T x C (RNN/transformer)
            # these are generated by rnn.pad_sequence internally
            one_hot = one_hot.permute(0, 2, 1) 
            is_exon = is_exon.permute(0, 2, 1)
            
            B,C,T = is_exon.shape # batch, channels, length
    
            # TODO: handle multiple meta channels (need to think carefully about joint masking)
            meta = torch.zeros( B, 2, T + rf * 2, device = self.device)  # one hot, zero for missing
            meta[:, 0, rf:-rf] = is_exon[:,0,:]
            meta[:, 1, rf:-rf] = (1.-is_exon[:,0,:])

            x = torch.concat( (meta, one_hot), 1)
            
            output = self.model(x.nan_to_num()) # spliceAI uses conv which want B x C x T

            mask = is_exon.isnan()

            one_hot_sub = one_hot[:, :, rf:-rf]
            seq_eval_mask = ~one_hot_sub.isnan()
            seq_out_norm = output - output.logsumexp(1, keepdims = True)
            loss = - (one_hot_sub[ seq_eval_mask ] * seq_out_norm[ seq_eval_mask ]).sum() / seq_eval_mask.sum() 
            
            #loss = tcn.my_bce_loss(seq_mask, mask, output, one_hot[:, :, rf:-rf]) 
            
            loss.backward()
            xm.optimizer_step(self.optimizer) if self.data_parallel else self.optimizer.step()
            #if self.xla and not self.data_parallel: 
            #    xm.mark_step()
            
            if self.xla: 
                tracker.add(self.batch_size)
                #if step_i % 10 == 0:
                xm.add_step_closure(self._train_update, args=(step_i, loss, tracker, epoch, one_hot.shape))
            else: 
                print(epoch, step_i, loss.item()) 

    def print(self, *args, **kwargs): 
        xm.master_print(*args, **kwargs) if self.xla else print(*args, **kwargs)
    
    def train(self):
        
        for epoch in range(1, 100 + 1):
            self.print('Epoch {} train begin {}'.format(epoch, time.strftime('%l:%M%p %Z on %b %d, %Y')))
            self.train_loop_fn(epoch)
            self.print('Epoch {} train end {}'.format(epoch, time.strftime('%l:%M%p %Z on %b %d, %Y')))
        
        if self.xla: 
            xm.wait_device_ops()
        
def _mp_fn(index, flags):
    # "cpu" # xm.xla_device() # == torch_xla.device()
    xla_ddp = TrainXLADDP(
        device = xm.xla_device() if flags["use_xla"] else "cpu",
        batch_size = flags["batch_size"],
        data_parallel = flags["data_parallel"]
    )
    xla_ddp.train()

if __name__ == '__main__':

    flags = { 
        "use_xla" : False,
        "batch_size" : 2, 
        "data_parallel" : False,
    } 
    
    if flags["data_parallel"]: 
        xmp.spawn(_mp_fn, args=(flags,))
    else: 
        _mp_fn(0, flags)

