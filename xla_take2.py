import torch_xla
from torch_xla import runtime as xr
import torch_xla.core.xla_model as xm
import torch_xla.distributed.parallel_loader as pl
import torch_xla.distributed.xla_multiprocessing as xmp

import time
import itertools

from pathlib import Path

import torch
import torchvision
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F

import transcript_data
import spliceAI
import numpy as np
import os
import tcn

import time

class CPURateTracker:
    def __init__(self):
        self.reset()

    def reset(self):
        self.start_time = time.time()
        self.count = 0

    def add(self, n=1):
        self.count += n

    def rate(self):
        elapsed_time = time.time() - self.start_time
        return self.count / elapsed_time if elapsed_time > 0 else 0.0

class TrainXLADDP():
    
    def __init__(
        self, 
        use_xla = False, 
        batch_size = 20, 
        data_parallel = False, 
        num_workers = 0,
        sequence_len = 10000
    ):
    
        self.device = xm.xla_device() if use_xla else "cpu" 
        self.batch_size = batch_size
        self.data_parallel = data_parallel
        self.sequence_len = sequence_len
        
        get_gene = transcript_data.get_generator(
            os.path.expanduser("hg38.fa.gz"), 
            "gencode.v24.annotation.gtf.gz",
            "ENCFF191YXW.tsv.gz") # neural cell polyA RNA-seq
        
        self.model = spliceAI.SpliceAI_10k(
            in_channels = 5, 
            out_channels = 4, 
            n_embed = 64
        ).to(self.device)
        
        train_chroms = ["chr%i" % i for i in range(2,23)] + ["chrX"]
        test_chroms = ["chr1"]
        
        # batch_size = 10. Cadaceus done 2^20 ~ 1M tokens per batch. So og is 10x smaller
        train_dataloader = transcript_data.get_dataloader(
            get_gene, 
            train_chroms, 
            receptive_field = 5000, 
            batch_size = self.batch_size, 
            num_workers = num_workers, 
            device = "cpu", # we pass cpu as the device since MpDeviceLoader handles the transfer to the TPU
            min_len = self.sequence_len, 
            max_len = self.sequence_len )
        self.train_device_loader = pl.MpDeviceLoader(train_dataloader, self.device) if self.xla else train_dataloader

        test_dataloader = transcript_data.get_dataloader(
            get_gene, 
            test_chroms, 
            receptive_field = 5000, 
            batch_size = self.batch_size, 
            num_workers = num_workers, 
            device = "cpu", 
            min_len = self.sequence_len, 
            max_len = self.sequence_len )
        self.test_device_loader = pl.MpDeviceLoader(test_dataloader, self.device) if self.xla else test_dataloader
        # could use bigger batch here but want to be consistent with mamba
        #test_dataloader = transcript_data.get_dataloader(get_gene, test_chroms, receptive_field = 5000, batch_size = 1, device = "cpu", max_len = 30000 )
        #self.test_device_loader = pl.MpDeviceLoader(test_dataloader, self.device)
        
        self.optimizer = torch.optim.Adam(self.model.parameters())

    @property
    def xla(self): 
        return not str(self.device) == "cpu" # TODO: handle CUDA
    
    def _train_update(self, step, loss, tracker, epoch):
        print(f'epoch: {epoch}, step: {step}, loss: {loss}, rate: {tracker.rate()}')
    
    def train_loop_fn(self, train, epoch):

        rf = self.model.receptive_field

        total_loss = torch.tensor(0., device = self.device) 
        
        tracker = xm.RateTracker() if self.xla else CPURateTracker()
        if train: 
            self.model.train()

        for step_i, dat in enumerate(self.train_device_loader if train else self.test_device_loader): 

            # we don't actually need lengths so could stop returning? 
            (is_exon, _), (one_hot, lengths), (one_hot_masked, _), (seq_mask, _), weights = dat
            #torch.save([is_exon, one_hot], cache_dir / f"{step_i}.pt")

            # maybe not needed now? 
            is_exon = is_exon.nan_to_num() # length 10000
            one_hot = one_hot.nan_to_num() # length 20000

            #if train: 
            self.optimizer.zero_grad()
            
            # convert to B x C x T (CNN) from B x T x C (RNN/transformer)
            # these are generated by rnn.pad_sequence internally
            one_hot = one_hot.permute(0, 2, 1) 
            is_exon = is_exon.permute(0, 2, 1)
            one_hot_masked = one_hot_masked.permute(0, 2, 1)
            # seq_mask = seq_mask.permute(0, 2, 1) # B x T so don't need to do this
            seq_mask_ = seq_mask[:, None, rf:-rf].float() # .expand(-1, 4, -1)
            
            # TODO: handle multiple meta channels (need to think carefully about joint masking)
            meta = F.pad(is_exon, (rf,rf)) # just pad seq dimension

            x = torch.concat( (meta, one_hot_masked), 1)
            
            output = self.model(x) # spliceAI uses conv which needs B x C x T

            one_hot_sub = one_hot[:, :, rf:-rf]
            
            seq_out_norm = output - output.logsumexp(1, keepdims = True)
            loss = - (seq_mask_ * one_hot_sub * seq_out_norm).sum() / (seq_mask_.sum() + 1e-8) 

            if train: 
                loss.backward()
            xm.optimizer_step(self.optimizer) if self.data_parallel else self.optimizer.step()
            #if self.xla and not self.data_parallel: 
            #   xm.mark_step()

            total_loss += loss
            tracker.add(self.batch_size)

            if step_i % 10 == 0:
                if self.xla:     
                    xm.add_step_closure(self._train_update, args=(step_i, loss, tracker, epoch))
                else: 
                    self._train_update(step_i, loss, tracker, epoch)

        total_loss = xm.all_reduce(xm.REDUCE_SUM, total_loss).item()
        return total_loss
    
    def print(self, *args, **kwargs): 
        xm.master_print(*args, **kwargs) if self.xla else print(*args, **kwargs)
    
    def train(self):
        
        for epoch in range(10):
            xm.set_rng_state(epoch, device = self.device)
            loss = self.train_loop_fn(True, epoch)

            xm.set_rng_state(42, device = self.device)
            test_loss = self.train_loop_fn(False, epoch)
            self.print('Epoch {} train loss {} test loss {} end {}'.format(epoch, loss, test_loss, time.strftime('%l:%M%p %Z on %b %d, %Y')))
        
        if self.xla: 
            xm.wait_device_ops()
        
def _mp_fn(index, flags):
    # "cpu" # xm.xla_device() # == torch_xla.device()
    xla_ddp = TrainXLADDP(**flags)
    xla_ddp.train()

if __name__ == '__main__':

    if False: 
        flags = { 
            "use_xla" : False,
            "batch_size" : 2, 
            "data_parallel" : False,
            "num_workers" : 0
        } 
    else:     
        flags = { 
            "use_xla" : True,
            "batch_size" : 50, 
            "data_parallel" : False,
            "num_workers" : 0
        } 

    print(flags)
    
    if flags["data_parallel"]: 
        xmp.spawn(_mp_fn, args=(flags,))
    else: 
        _mp_fn(0, flags)

if False: 
    cache_dir = Path("data_cache")
    
    is_exon_list = []
    one_hot_list = []
    
    for step_i in range(138): 
        is_exon, one_hot = torch.load(cache_dir / f"{step_i}.pt")
        is_exon_list.append(is_exon)
        one_hot_list.append(one_hot)
    
    is_exon_comb = torch.concatenate(is_exon_list)
    one_hot_comb = torch.concatenate(one_hot_list)
    
    torch.save([is_exon_comb,one_hot_comb], "data_cache.pt")
    
    import torch
    is_exon_comb,one_hot_comb = torch.load("data_cache.pt")
    is_exon_comb = is_exon_comb.nan_to_num()
    one_hot_comb = one_hot_comb.nan_to_num()
    torch.save([is_exon_comb,one_hot_comb], "data_cache_clean.pt")



